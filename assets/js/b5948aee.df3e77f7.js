"use strict";(self.webpackChunkmy_tfg_logbook=self.webpackChunkmy_tfg_logbook||[]).push([[7304],{7938:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>r,contentTitle:()=>a,default:()=>u,frontMatter:()=>l,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"Running_LLMs/First_LLMs_Test_On_Google_Colab","title":"Testing and using Google Colab and Hugging Face for LLMs in Image Analysis and glaucoma detection","description":"Purpose","source":"@site/docs/4. Running_LLMs/2. First_LLMs_Test_On_Google_Colab.md","sourceDirName":"4. Running_LLMs","slug":"/Running_LLMs/First_LLMs_Test_On_Google_Colab","permalink":"/My-TFG-Logbook/docs/Running_LLMs/First_LLMs_Test_On_Google_Colab","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"Introduction","permalink":"/My-TFG-Logbook/docs/Running_LLMs/Introduction"}}');var o=i(4848),t=i(8453);const l={sidebar_position:1},a="Testing and using Google Colab and Hugging Face for LLMs in Image Analysis and glaucoma detection",r={},d=[{value:"Purpose",id:"purpose",level:2},{value:"Key Sections and Code Execution",id:"key-sections-and-code-execution",level:2},{value:"Observations from Outputs",id:"observations-from-outputs",level:2},{value:"Conclusion and Further Considerations",id:"conclusion-and-further-considerations",level:2}];function c(e){const n={br:"br",code:"code",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"testing-and-using-google-colab-and-hugging-face-for-llms-in-image-analysis-and-glaucoma-detection",children:"Testing and using Google Colab and Hugging Face for LLMs in Image Analysis and glaucoma detection"})}),"\n",(0,o.jsx)(n.h2,{id:"purpose",children:"Purpose"}),"\n",(0,o.jsx)(n.p,{children:"This notebook demonstrates the use of two language models within Google Colab for potential application in image analysis, specifically in glaucoma detection. The models used include:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"CLIP"})," for image processing using a ViT-based architecture."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Flan-T5"})," as an alternative lightweight language model to Llama 2 and 3."]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"Note: Fine-tuning was not performed with images in this notebook."}),"\n",(0,o.jsx)(n.h2,{id:"key-sections-and-code-execution",children:"Key Sections and Code Execution"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Library Installation:"})}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["Installs the necessary libraries such as ",(0,o.jsx)(n.code,{children:"transformers"}),", ",(0,o.jsx)(n.code,{children:"torch"}),", and ",(0,o.jsx)(n.code,{children:"ipywidgets"})," for model operations and interactive widgets."]}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Adding Hugging Face Token:"})}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Authenticates with Hugging Face API to enable access to hosted models."}),"\n",(0,o.jsx)(n.li,{children:"Token is initially embedded in the notebook but can be set privately for security."}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Model Loading:"})}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Loads and configures CLIP (for image analysis) and Flan-T5 (for text generation)."}),"\n",(0,o.jsx)(n.li,{children:"The notebook includes a function for text input processing and image processing via CLIP with Flan-T5 generating descriptive outputs."}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Widget-based Interface for Google Colab:"})}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Interactive widgets are set up to allow text input and image upload."}),"\n",(0,o.jsx)(n.li,{children:"A simple interface allows the user to interact with models, receiving responses from Flan-T5."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"observations-from-outputs",children:"Observations from Outputs"}),"\n",(0,o.jsx)(n.p,{children:"When testing the model's responses, it was observed that:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:'The model replied with a simple "no" to the question of whether the image showed signs of glaucoma. This response suggests limitations in using such models without fine-tuning for medical image analysis.'}),"\n",(0,o.jsxs)(n.li,{children:["Limitations might arise due to:","\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Model selection not being optimized for detailed responses."}),"\n",(0,o.jsx)(n.li,{children:"Lack of specialized prompts guiding the LLM to extract relevant features from the image."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"conclusion-and-further-considerations",children:"Conclusion and Further Considerations"}),"\n",(0,o.jsx)(n.p,{children:"Based on the limited response, the chosen models, even with fine-tuning, may not yield detailed diagnostic insights for glaucoma detection. It suggests that LLMs, as configured here, might not be reliable for such tasks without more specific adaptations."}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Suggested Next Steps:"}),(0,o.jsx)(n.br,{}),"\n","To further explore the application of LLMs in medical image analysis, multiple images could be tested, and different model architectures could be evaluated. Additionally, advanced prompt engineering and detailed model tuning should be considered for accurate results."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Question:"}),(0,o.jsx)(n.br,{}),"\n","Could this approach of testing various images and model configurations be a valuable part of the final thesis, even if it does not provide definitive clinical insight? We will find it out in the next pages :)"]}),"\n",(0,o.jsx)(n.p,{children:"For a full"})]})}function u(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(c,{...e})}):c(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>l,x:()=>a});var s=i(6540);const o={},t=s.createContext(o);function l(e){const n=s.useContext(t);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:l(e.components),s.createElement(t.Provider,{value:n},e.children)}}}]);