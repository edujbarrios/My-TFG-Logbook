---
sidebar_position: 3
---
## Clarifications

At this section I will be downloding the sofware, this means doing the git clone and the readme instructuions.

> üìù **Note:** Remember that the purpose of this logbook is to serve as an actual logbook. This means that I will document here the entire installation process of everything I use, including any potential issues. This way, I can easily reference error messages or troubleshooting steps to discuss with my tutoring professors. **Of course, I do not doubt the capabilities of any readers of this document.**


## Following README installation 


First lets start by glit clonning the repository: https://github.com/JoshuaChou2018/SkinGPT-4

Once downloaded, this software uses **280 Mb.**

## Docker Setup

To encapsulate everything, I have decided to sum up the installing steps in a dockerfile, for better handling and to avoid further problems, the dockerfile i used is the following:

```bash
# Use a base image with CUDA support
FROM nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu20.04

# Configure environment variables for non-interactive installation and set PATH for Conda
ENV DEBIAN_FRONTEND=noninteractive
ENV PATH="/opt/conda/bin:$PATH"

# Install essential system dependencies
RUN apt-get update && \
    apt-get install -y \
    wget \
    git \
    libgl1-mesa-glx \
    libglib2.0-0 \
    && rm -rf /var/lib/apt/lists/*

# Install Miniconda
RUN wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O miniconda.sh && \
    bash miniconda.sh -b -p /opt/conda && \
    rm miniconda.sh && \
    /opt/conda/bin/conda init

# Step 1: Create a Conda environment from environment.yml at the project root
# Place environment.yml in the root folder with Dockerfile
COPY environment.yml /workspace/environment.yml
RUN /opt/conda/bin/conda env create -f /workspace/environment.yml && \
    /opt/conda/bin/conda clean -a -y

# Set the shell to activate the Conda environment
SHELL ["conda", "run", "-n", "skingpt4_llama2", "/bin/bash", "-c"]

# Step 2: Install additional dependencies using mamba and pip
RUN conda install -c conda-forge mamba=1.4.7 && \
    conda install -y pytorch==2.0.0 torchvision==0.15.0 torchaudio==2.0.0 pytorch-cuda=11.8 -c pytorch -c nvidia && \
    pip install git+https://github.com/lm-sys/FastChat.git@v0.1.10 && \
    pip install transformers==4.28.0

# Step 3: Clone necessary model repositories into /workspace (container location)
WORKDIR /workspace
RUN git clone https://huggingface.co/meta-llama/Llama-2-13b-chat-hf && \
    git lfs install && \
    git clone https://huggingface.co/lmsys/vicuna-13b-delta-v0 && \
    git clone https://huggingface.co/huggyllama/llama-13b

# Step 4: Apply delta weights for Vicuna model
RUN python -m fastchat.model.apply_delta --base ./llama-13b --target ./vicuna --delta ./vicuna-13b-delta-v0

# Step 5: Copy the main project code into the container (root structure should match on host)
# ‚îú‚îÄ‚îÄ Dockerfile
# ‚îú‚îÄ‚îÄ environment.yml
# ‚îú‚îÄ‚îÄ demo.py
# ‚îú‚îÄ‚îÄ eval_configs/
# ‚îú‚îÄ‚îÄ skingpt4/
# ‚îú‚îÄ‚îÄ weights/ (for model weights)
COPY . /workspace/SkinGPT-4-llama2

# Step 6: Optionally, update paths in configuration YAML files if necessary
# For example, you can modify paths using `sed` if any YAML files require absolute paths to model directories.
# Example (uncomment and adjust as needed):
# RUN sed -i 's|path/to/llama|/workspace/meta-llama/Llama-2-13b-chat-hf|' /workspace/SkinGPT-4-llama2/skingpt4/configs/models/skingpt4_llama2_13bchat.yaml

# Default command to launch the demo with Llama2 configuration
CMD ["conda", "run", "-n", "skingpt4_llama2", "python", "demo.py", "--cfg-path", "eval_configs/skingpt4_eval_llama2_13bchat.yaml", "--gpu-id", "0"]

# --- Project Structure Requirements ---

# Root Project Directory:
# ‚îú‚îÄ‚îÄ Dockerfile            # Place Dockerfile at the root
# ‚îú‚îÄ‚îÄ environment.yml       # Place Conda environment file at the root
# ‚îú‚îÄ‚îÄ demo.py               # Main demo script
# ‚îú‚îÄ‚îÄ eval_configs/         # Configuration files for evaluation
# ‚îú‚îÄ‚îÄ skingpt4/             # Main code directory with subfolders
# ‚îú‚îÄ‚îÄ weights/              # Folder for model weights (trained .pth files)
# ‚îî‚îÄ‚îÄ additional files      # Other files, such as README.md, images, prompts

# --- Build and Run Instructions ---

# To build the Docker image:
# $ docker build -t skingpt4_image .

# To run the Docker container with GPU support:
# $ docker run --gpus all -it skingpt4_image

# This Dockerfile prepares the environment, installs dependencies, and sets up the demo with the Llama2 configuration.
# To use a different configuration (e.g., Vicuna), modify the `CMD` to point to the appropriate YAML file.
```

### Step-by-Step Guide to Setting Up the Docker Container in Docker Desktop

To detect and fix any potential installation issues, it's often best to open Docker Desktop, access the terminal, and execute each command step-by-step. This approach allows you to verify that each part of the installation works correctly and identify any errors early on.

#### Step-by-Step Guide to Running the Dockerfile Commands in Docker Desktop

1. **Start an Interactive Container**:
   Begin by creating a container interactively from the base image. This allows you to run each command and troubleshoot as needed:
   ```bash
   docker run --gpus all -it nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu20.04 /bin/bash
   ```
   Once inside the container, you can proceed with each command from the Dockerfile.

2. **Set Environment Variables**:
   Configure necessary environment variables for non-interactive installs and the Conda path:
   ```bash
   export DEBIAN_FRONTEND=noninteractive
   export PATH="/opt/conda/bin:$PATH"
   ```

3. **Install System Dependencies**:
   Install essential system packages required for the project:
   ```bash
   apt-get update && \
   apt-get install -y wget git libgl1-mesa-glx libglib2.0-0 && \
   rm -rf /var/lib/apt/lists/*
   ```

4. **Install Miniconda**:
   Download and install Miniconda, which you will use to create the Conda environment:
   ```bash
   wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O miniconda.sh
   bash miniconda.sh -b -p /opt/conda
   rm miniconda.sh
   /opt/conda/bin/conda init
   ```

5. **Create the Conda Environment**:
   Ensure you have your `environment.yml` file in a shared directory accessible from Docker Desktop, or copy its contents here directly. Then, create the environment as specified:
   ```bash
   conda env create -f /path/to/environment.yml
   conda clean -a -y
   ```

6. **Install Additional Dependencies with Mamba and Pip**:
   Once the Conda environment is created, install the additional dependencies using `mamba` and `pip`:
   ```bash
   conda install -c conda-forge mamba=1.4.7 && \
   conda install -y pytorch==2.0.0 torchvision==0.15.0 torchaudio==2.0.0 pytorch-cuda=11.8 -c pytorch -c nvidia && \
   pip install git+https://github.com/lm-sys/FastChat.git@v0.1.10 && \
   pip install transformers==4.28.0
   ```

7. **Clone Necessary Git Repositories**:
   Clone the required repositories to set up model weights and configurations:
   ```bash
   git clone https://huggingface.co/meta-llama/Llama-2-13b-chat-hf
   git lfs install
   git clone https://huggingface.co/lmsys/vicuna-13b-delta-v0
   git clone https://huggingface.co/huggyllama/llama-13b
   ```

8. **Apply Delta Weights to the Vicuna Model**:
   Apply the delta weights to create the final Vicuna model. This requires running a Python script from the FastChat library:
   ```bash
   python -m fastchat.model.apply_delta --base ./llama-13b --target ./vicuna --delta ./vicuna-13b-delta-v0
   ```

9. **Copy Project Files**:
   Use Docker Desktop‚Äôs file-sharing feature or the `docker cp` command to copy the project files from your local machine into the container. You should copy the following structure:
   ```
   ‚îú‚îÄ‚îÄ Dockerfile             # Placed at the project root
   ‚îú‚îÄ‚îÄ environment.yml        # Conda environment file
   ‚îú‚îÄ‚îÄ demo.py                # Main demo script
   ‚îú‚îÄ‚îÄ eval_configs/          # Configuration files for evaluation
   ‚îú‚îÄ‚îÄ skingpt4/              # Main code directory with subfolders
   ‚îú‚îÄ‚îÄ weights/               # Folder for model weights (trained .pth files)
   ‚îî‚îÄ‚îÄ additional files       # Other files, such as README.md, images, and prompts
   ```

10. **Test the Setup**:
    Once everything is installed, run the demo script manually to ensure everything is set up correctly:
    ```bash
    python demo.py --cfg-path eval_configs/skingpt4_eval_llama2_13bchat.yaml --gpu-id 0
    ```

11. **Save the Container as an Image**:
    If all steps work without issues, you can save this container as a Docker image. In a separate terminal, run:
    ```bash
    docker commit <container_id> skingpt4_image
    ```

By following this interactive approach in Docker Desktop, you can ensure that each installation step completes successfully and identify any issues immediately. Once you‚Äôve confirmed everything works, you can proceed to build the Dockerfile directly, which should now work smoothly based on your manual verification.
