"use strict";(self.webpackChunkmy_tfg_logbook=self.webpackChunkmy_tfg_logbook||[]).push([[9358],{4896:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"SkinGPT4/Formal Installation","title":"Formal Installation","description":"Clarifications","source":"@site/docs/4. SkinGPT4/3. Formal Installation.md","sourceDirName":"4. SkinGPT4","slug":"/SkinGPT4/Formal Installation","permalink":"/My-TFG-Logbook/es/docs/SkinGPT4/Formal Installation","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"Environment Details","permalink":"/My-TFG-Logbook/es/docs/SkinGPT4/Enviroment"},"next":{"title":"Step-by-Step Guide to Setting Up the Docker Container in Docker Desktop","permalink":"/My-TFG-Logbook/es/docs/SkinGPT4/Step_by_Step_Docker_Setup_Guide"}}');var o=t(4848),a=t(8453);const r={sidebar_position:3},s=void 0,l={},c=[{value:"Clarifications",id:"clarifications",level:2},{value:"Following README installation",id:"following-readme-installation",level:2},{value:"Docker Setup",id:"docker-setup",level:2},{value:"Step-by-Step Guide to Setting Up the Docker Container in Docker Desktop",id:"step-by-step-guide-to-setting-up-the-docker-container-in-docker-desktop",level:3},{value:"Step-by-Step Guide to Running the Dockerfile Commands in Docker Desktop",id:"step-by-step-guide-to-running-the-dockerfile-commands-in-docker-desktop",level:4}];function d(e){const n={a:"a",blockquote:"blockquote",code:"code",h2:"h2",h3:"h3",h4:"h4",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,a.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.h2,{id:"clarifications",children:"Clarifications"}),"\n",(0,o.jsx)(n.p,{children:"At this section I will be downloding the sofware, this means doing the git clone and the readme instructuions."}),"\n",(0,o.jsxs)(n.blockquote,{children:["\n",(0,o.jsxs)(n.p,{children:["\ud83d\udcdd ",(0,o.jsx)(n.strong,{children:"Note:"})," Remember that the purpose of this logbook is to serve as an actual logbook. This means that I will document here the entire installation process of everything I use, including any potential issues. This way, I can easily reference error messages or troubleshooting steps to discuss with my tutoring professors. ",(0,o.jsx)(n.strong,{children:"Of course, I do not doubt the capabilities of any readers of this document."})]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"following-readme-installation",children:"Following README installation"}),"\n",(0,o.jsxs)(n.p,{children:["First lets start by glit clonning the repository: ",(0,o.jsx)(n.a,{href:"https://github.com/JoshuaChou2018/SkinGPT-4",children:"https://github.com/JoshuaChou2018/SkinGPT-4"})]}),"\n",(0,o.jsxs)(n.p,{children:["Once downloaded, this software uses ",(0,o.jsx)(n.strong,{children:"280 Mb."})]}),"\n",(0,o.jsx)(n.h2,{id:"docker-setup",children:"Docker Setup"}),"\n",(0,o.jsx)(n.p,{children:"To encapsulate everything, I have decided to sum up the installing steps in a dockerfile, for better handling and to avoid further problems, the dockerfile i used is the following:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:'# Use a base image with CUDA support\r\nFROM nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu20.04\r\n\r\n# Configure environment variables for non-interactive installation and set PATH for Conda\r\nENV DEBIAN_FRONTEND=noninteractive\r\nENV PATH="/opt/conda/bin:$PATH"\r\n\r\n# Install essential system dependencies\r\nRUN apt-get update && \\\r\n    apt-get install -y \\\r\n    wget \\\r\n    git \\\r\n    libgl1-mesa-glx \\\r\n    libglib2.0-0 \\\r\n    && rm -rf /var/lib/apt/lists/*\r\n\r\n# Install Miniconda\r\nRUN wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O miniconda.sh && \\\r\n    bash miniconda.sh -b -p /opt/conda && \\\r\n    rm miniconda.sh && \\\r\n    /opt/conda/bin/conda init\r\n\r\n# Step 1: Create a Conda environment from environment.yml at the project root\r\n# Place environment.yml in the root folder with Dockerfile\r\nCOPY environment.yml /workspace/environment.yml\r\nRUN /opt/conda/bin/conda env create -f /workspace/environment.yml && \\\r\n    /opt/conda/bin/conda clean -a -y\r\n\r\n# Set the shell to activate the Conda environment\r\nSHELL ["conda", "run", "-n", "skingpt4_llama2", "/bin/bash", "-c"]\r\n\r\n# Step 2: Install additional dependencies using mamba and pip\r\nRUN conda install -c conda-forge mamba=1.4.7 && \\\r\n    conda install -y pytorch==2.0.0 torchvision==0.15.0 torchaudio==2.0.0 pytorch-cuda=11.8 -c pytorch -c nvidia && \\\r\n    pip install git+https://github.com/lm-sys/FastChat.git@v0.1.10 && \\\r\n    pip install transformers==4.28.0\r\n\r\n# Step 3: Clone necessary model repositories into /workspace (container location)\r\nWORKDIR /workspace\r\nRUN git clone https://huggingface.co/meta-llama/Llama-2-13b-chat-hf && \\\r\n    git lfs install && \\\r\n    git clone https://huggingface.co/lmsys/vicuna-13b-delta-v0 && \\\r\n    git clone https://huggingface.co/huggyllama/llama-13b\r\n\r\n# Step 4: Apply delta weights for Vicuna model\r\nRUN python -m fastchat.model.apply_delta --base ./llama-13b --target ./vicuna --delta ./vicuna-13b-delta-v0\r\n\r\n# Step 5: Copy the main project code into the container (root structure should match on host)\r\n# \u251c\u2500\u2500 Dockerfile\r\n# \u251c\u2500\u2500 environment.yml\r\n# \u251c\u2500\u2500 demo.py\r\n# \u251c\u2500\u2500 eval_configs/\r\n# \u251c\u2500\u2500 skingpt4/\r\n# \u251c\u2500\u2500 weights/ (for model weights)\r\nCOPY . /workspace/SkinGPT-4-llama2\r\n\r\n# Step 6: Optionally, update paths in configuration YAML files if necessary\r\n# For example, you can modify paths using `sed` if any YAML files require absolute paths to model directories.\r\n# Example (uncomment and adjust as needed):\r\n# RUN sed -i \'s|path/to/llama|/workspace/meta-llama/Llama-2-13b-chat-hf|\' /workspace/SkinGPT-4-llama2/skingpt4/configs/models/skingpt4_llama2_13bchat.yaml\r\n\r\n# Default command to launch the demo with Llama2 configuration\r\nCMD ["conda", "run", "-n", "skingpt4_llama2", "python", "demo.py", "--cfg-path", "eval_configs/skingpt4_eval_llama2_13bchat.yaml", "--gpu-id", "0"]\r\n\r\n# --- Project Structure Requirements ---\r\n\r\n# Root Project Directory:\r\n# \u251c\u2500\u2500 Dockerfile            # Place Dockerfile at the root\r\n# \u251c\u2500\u2500 environment.yml       # Place Conda environment file at the root\r\n# \u251c\u2500\u2500 demo.py               # Main demo script\r\n# \u251c\u2500\u2500 eval_configs/         # Configuration files for evaluation\r\n# \u251c\u2500\u2500 skingpt4/             # Main code directory with subfolders\r\n# \u251c\u2500\u2500 weights/              # Folder for model weights (trained .pth files)\r\n# \u2514\u2500\u2500 additional files      # Other files, such as README.md, images, prompts\r\n\r\n# --- Build and Run Instructions ---\r\n\r\n# To build the Docker image:\r\n# $ docker build -t skingpt4_image .\r\n\r\n# To run the Docker container with GPU support:\r\n# $ docker run --gpus all -it skingpt4_image\r\n\r\n# This Dockerfile prepares the environment, installs dependencies, and sets up the demo with the Llama2 configuration.\r\n# To use a different configuration (e.g., Vicuna), modify the `CMD` to point to the appropriate YAML file.\n'})}),"\n",(0,o.jsx)(n.h3,{id:"step-by-step-guide-to-setting-up-the-docker-container-in-docker-desktop",children:"Step-by-Step Guide to Setting Up the Docker Container in Docker Desktop"}),"\n",(0,o.jsx)(n.p,{children:"To detect and fix any potential installation issues, it's often best to open Docker Desktop, access the terminal, and execute each command step-by-step. This approach allows you to verify that each part of the installation works correctly and identify any errors early on."}),"\n",(0,o.jsx)(n.h4,{id:"step-by-step-guide-to-running-the-dockerfile-commands-in-docker-desktop",children:"Step-by-Step Guide to Running the Dockerfile Commands in Docker Desktop"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Start an Interactive Container"}),":\r\nBegin by creating a container interactively from the base image. This allows you to run each command and troubleshoot as needed:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"docker run --gpus all -it nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu20.04 /bin/bash\n"})}),"\n",(0,o.jsx)(n.p,{children:"Once inside the container, you can proceed with each command from the Dockerfile."}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Set Environment Variables"}),":\r\nConfigure necessary environment variables for non-interactive installs and the Conda path:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:'export DEBIAN_FRONTEND=noninteractive\r\nexport PATH="/opt/conda/bin:$PATH"\n'})}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Install System Dependencies"}),":\r\nInstall essential system packages required for the project:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"apt-get update && \\\r\napt-get install -y wget git libgl1-mesa-glx libglib2.0-0 && \\\r\nrm -rf /var/lib/apt/lists/*\n"})}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Install Miniconda"}),":\r\nDownload and install Miniconda, which you will use to create the Conda environment:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O miniconda.sh\r\nbash miniconda.sh -b -p /opt/conda\r\nrm miniconda.sh\r\n/opt/conda/bin/conda init\n"})}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Create the Conda Environment"}),":\r\nEnsure you have your ",(0,o.jsx)(n.code,{children:"environment.yml"})," file in a shared directory accessible from Docker Desktop, or copy its contents here directly. Then, create the environment as specified:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"conda env create -f /path/to/environment.yml\r\nconda clean -a -y\n"})}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Install Additional Dependencies with Mamba and Pip"}),":\r\nOnce the Conda environment is created, install the additional dependencies using ",(0,o.jsx)(n.code,{children:"mamba"})," and ",(0,o.jsx)(n.code,{children:"pip"}),":"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"conda install -c conda-forge mamba=1.4.7 && \\\r\nconda install -y pytorch==2.0.0 torchvision==0.15.0 torchaudio==2.0.0 pytorch-cuda=11.8 -c pytorch -c nvidia && \\\r\npip install git+https://github.com/lm-sys/FastChat.git@v0.1.10 && \\\r\npip install transformers==4.28.0\n"})}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Clone Necessary Git Repositories"}),":\r\nClone the required repositories to set up model weights and configurations:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"git clone https://huggingface.co/meta-llama/Llama-2-13b-chat-hf\r\ngit lfs install\r\ngit clone https://huggingface.co/lmsys/vicuna-13b-delta-v0\r\ngit clone https://huggingface.co/huggyllama/llama-13b\n"})}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Apply Delta Weights to the Vicuna Model"}),":\r\nApply the delta weights to create the final Vicuna model. This requires running a Python script from the FastChat library:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"python -m fastchat.model.apply_delta --base ./llama-13b --target ./vicuna --delta ./vicuna-13b-delta-v0\n"})}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Copy Project Files"}),":\r\nUse Docker Desktop\u2019s file-sharing feature or the ",(0,o.jsx)(n.code,{children:"docker cp"})," command to copy the project files from your local machine into the container. You should copy the following structure:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"\u251c\u2500\u2500 Dockerfile             # Placed at the project root\r\n\u251c\u2500\u2500 environment.yml        # Conda environment file\r\n\u251c\u2500\u2500 demo.py                # Main demo script\r\n\u251c\u2500\u2500 eval_configs/          # Configuration files for evaluation\r\n\u251c\u2500\u2500 skingpt4/              # Main code directory with subfolders\r\n\u251c\u2500\u2500 weights/               # Folder for model weights (trained .pth files)\r\n\u2514\u2500\u2500 additional files       # Other files, such as README.md, images, and prompts\n"})}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Test the Setup"}),":\r\nOnce everything is installed, run the demo script manually to ensure everything is set up correctly:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"python demo.py --cfg-path eval_configs/skingpt4_eval_llama2_13bchat.yaml --gpu-id 0\n"})}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Save the Container as an Image"}),":\r\nIf all steps work without issues, you can save this container as a Docker image. In a separate terminal, run:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"docker commit <container_id> skingpt4_image\n"})}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"By following this interactive approach in Docker Desktop, you can ensure that each installation step completes successfully and identify any issues immediately. Once you\u2019ve confirmed everything works, you can proceed to build the Dockerfile directly, which should now work smoothly based on your manual verification."})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>s});var i=t(6540);const o={},a=i.createContext(o);function r(e){const n=i.useContext(a);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),i.createElement(a.Provider,{value:n},e.children)}}}]);