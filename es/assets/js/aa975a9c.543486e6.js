"use strict";(self.webpackChunkmy_tfg_logbook=self.webpackChunkmy_tfg_logbook||[]).push([[6287],{2956:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>u,frontMatter:()=>r,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"Introducing VLLMs/VLLMs_Notebook_Summary_with_Introduction","title":"VLLMs_Notebook_Summary_with_Introduction","description":"Introduction to Vision Language Models (VLLMs)","source":"@site/docs/5. Introducing VLLMs/VLLMs_Notebook_Summary_with_Introduction.md","sourceDirName":"5. Introducing VLLMs","slug":"/Introducing VLLMs/VLLMs_Notebook_Summary_with_Introduction","permalink":"/My-TFG-Logbook/es/docs/Introducing VLLMs/VLLMs_Notebook_Summary_with_Introduction","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"Exploring Google Colab Combined with Gemini for Project Use","permalink":"/My-TFG-Logbook/es/docs/Initial Research/exploring_gemini_colab"}}');var o=i(4848),s=i(8453);const r={sidebar_position:1},a=void 0,l={},c=[{value:"Introduction to Vision Language Models (VLLMs)",id:"introduction-to-vision-language-models-vllms",level:2},{value:"Hardware Requirements and Limitations",id:"hardware-requirements-and-limitations",level:3}];function d(e){const n={h2:"h2",h3:"h3",li:"li",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.h2,{id:"introduction-to-vision-language-models-vllms",children:"Introduction to Vision Language Models (VLLMs)"}),"\n",(0,o.jsx)(n.p,{children:"Vision Language Models (VLLMs) represent a class of models that combine vision and language processing abilities.\nThese models are designed to interpret and generate human-readable descriptions from visual data (such as images)\nby leveraging extensive pre-trained architectures that process both text and image data. Typical examples include models\nlike CLIP, which matches images with text descriptions, and multi-modal transformers like Flan-T5, which can generate\ntext outputs based on prompts or features extracted from images."}),"\n",(0,o.jsx)(n.h3,{id:"hardware-requirements-and-limitations",children:"Hardware Requirements and Limitations"}),"\n",(0,o.jsx)(n.p,{children:"VLLMs often require significant computational power to function effectively, particularly for tasks involving high-resolution\nimages or complex language processing tasks. The hardware limitations for working with these models include:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"GPU Requirements"}),":\nVLLMs frequently require GPUs with high memory capacity (at least 16GB VRAM) to handle large batches of image and text data.\nGPUs from NVIDIA\u2019s A100 or T4 line, commonly available on platforms like Google Colab, are ideal for running these models.\nHigher-end GPUs facilitate faster processing and support larger model versions."]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"RAM Constraints"}),":\nRunning and fine-tuning large VLLMs, especially with models that include billions of parameters (e.g., models like LLaMA 3.2 with 7 billion parameters),\ncan require upwards of 24GB of RAM. Lower RAM capacities can lead to slower processing times, increased latency, and the inability\nto handle very large models without memory errors."]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Inference and Fine-Tuning Considerations"}),":\nFine-tuning a VLLM typically demands more resources than standard inference (using a model without modification). The fine-tuning process\ninvolves adjusting the model\u2019s internal weights and requires both more memory and prolonged GPU access.\nAccess to high-performance cloud instances, such as Colab Pro, can mitigate these limitations, offering faster and more efficient fine-tuning processes."]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"These hardware requirements often limit accessibility to VLLMs, especially for personal or limited-budget projects.\nEfficient memory management and model optimization strategies are necessary to deploy VLLMs effectively without overwhelming available resources."})]})}function u(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>a});var t=i(6540);const o={},s=t.createContext(o);function r(e){const n=t.useContext(s);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);