<!doctype html>
<html lang="es" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-Exploring ZSL/ZSL_applied_to_glaucoma_images" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.6.0">
<title data-rh="true">Exploring Zero-Shot Learning with CLIP for Glaucoma Detection | LLMs and Fine Tuning in glaucoma detection</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://edujbarrios.github.io/My-TFG-Logbook/es/img/social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://edujbarrios.github.io/My-TFG-Logbook/es/img/social-card.jpg"><meta data-rh="true" property="og:url" content="https://edujbarrios.github.io/My-TFG-Logbook/es/docs/Exploring ZSL/ZSL_applied_to_glaucoma_images"><meta data-rh="true" property="og:locale" content="es"><meta data-rh="true" property="og:locale:alternate" content="en"><meta data-rh="true" name="docusaurus_locale" content="es"><meta data-rh="true" name="docsearch:language" content="es"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Exploring Zero-Shot Learning with CLIP for Glaucoma Detection | LLMs and Fine Tuning in glaucoma detection"><meta data-rh="true" name="description" content="In this document, I present an exploration into the potential of using zero-shot learning with an adapted version of CLIP (Contrastive Language–Image Pre-training), fine-tuned to detect glaucoma in retinographies. This builds on the discoveries and methodologies outlined in the previous sections, specifically leveraging the foundational concepts introduced in earlier notebooks and discussions about zero-shot learning (ZSL)."><meta data-rh="true" property="og:description" content="In this document, I present an exploration into the potential of using zero-shot learning with an adapted version of CLIP (Contrastive Language–Image Pre-training), fine-tuned to detect glaucoma in retinographies. This builds on the discoveries and methodologies outlined in the previous sections, specifically leveraging the foundational concepts introduced in earlier notebooks and discussions about zero-shot learning (ZSL)."><link data-rh="true" rel="icon" href="/My-TFG-Logbook/es/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://edujbarrios.github.io/My-TFG-Logbook/es/docs/Exploring ZSL/ZSL_applied_to_glaucoma_images"><link data-rh="true" rel="alternate" href="https://edujbarrios.github.io/My-TFG-Logbook/docs/Exploring ZSL/ZSL_applied_to_glaucoma_images" hreflang="en"><link data-rh="true" rel="alternate" href="https://edujbarrios.github.io/My-TFG-Logbook/es/docs/Exploring ZSL/ZSL_applied_to_glaucoma_images" hreflang="es"><link data-rh="true" rel="alternate" href="https://edujbarrios.github.io/My-TFG-Logbook/docs/Exploring ZSL/ZSL_applied_to_glaucoma_images" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/My-TFG-Logbook/es/blog/rss.xml" title="LLMs and Fine Tuning in glaucoma detection RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/My-TFG-Logbook/es/blog/atom.xml" title="LLMs and Fine Tuning in glaucoma detection Atom Feed"><link rel="stylesheet" href="/My-TFG-Logbook/es/assets/css/styles.9760b7e3.css">
<script src="/My-TFG-Logbook/es/assets/js/runtime~main.2db26b1f.js" defer="defer"></script>
<script src="/My-TFG-Logbook/es/assets/js/main.4d09d551.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"dark")}(),function(){try{const a=new URLSearchParams(window.location.search).entries();for(var[t,e]of a)if(t.startsWith("docusaurus-data-")){var n=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(n,e)}}catch(t){}}(),document.documentElement.setAttribute("data-announcement-bar-initially-dismissed",function(){try{return"true"===localStorage.getItem("docusaurus.announcement.dismiss")}catch(t){}return!1}())</script><div id="__docusaurus"><div role="region" aria-label="Saltar al contenido principal"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Saltar al contenido principal</a></div><div class="announcementBar_mb4j" style="background-color:#541087;color:#FFFFFF" role="banner"><div class="announcementBarPlaceholder_vyr4"></div><div class="content_knG7 announcementBarContent_xLdY">⚠️ La versión en español de esta documentación se agregará pronto. ¡Gracias por tu paciencia!</div><button type="button" aria-label="Cerrar" class="clean-btn close closeButton_CVFx announcementBarClose_gvF7"><svg viewBox="0 0 15 15" width="14" height="14"><g stroke="currentColor" stroke-width="3.1"><path d="M.75.75l13.5 13.5M14.25.75L.75 14.25"></path></g></svg></button></div><nav aria-label="Principal" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Alternar barra lateral" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/My-TFG-Logbook/es/"><div class="navbar__logo"><img src="/My-TFG-Logbook/es/img/logo.png" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/My-TFG-Logbook/es/img/logo.png" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">LLMs and fine tuning in glaucoma detection </b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/My-TFG-Logbook/es/docs/">Docs</a><a class="navbar__item navbar__link" href="/My-TFG-Logbook/es/blog">Blog</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/edujbarrios" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Cambiar entre modo oscuro y claro (actualmente modo oscuro)" aria-label="Cambiar entre modo oscuro y claro (actualmente modo oscuro)" aria-live="polite" aria-pressed="true"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Volver al principio" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Barra lateral de Documentos" class="menu thin-scrollbar menu_SIkG menuWithAnnouncementBar_GW3s"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/My-TFG-Logbook/es/docs/">Launcher</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/My-TFG-Logbook/es/docs/Documentation Overview/intro">Documentation Overview</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/My-TFG-Logbook/es/docs/General Thoughts About This Thesis/How We Started">General Thoughts About This Thesis</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/My-TFG-Logbook/es/docs/Initial Research/initial-research">Initial Research</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" href="/My-TFG-Logbook/es/docs/Exploring ZSL/ZSL_applied_to_glaucoma_images">Exploring ZSL</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/My-TFG-Logbook/es/docs/Exploring ZSL/ZSL_applied_to_glaucoma_images">Exploring Zero-Shot Learning with CLIP for Glaucoma Detection</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/My-TFG-Logbook/es/docs/Exploring ZSL/State_Of_The_Art_Related_To_Glaucoma">Is there any similar approach based in glaucoma using ZSL technique ?</a></li></ul></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Rastro de navegación"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Página de Inicio" class="breadcrumbs__link" href="/My-TFG-Logbook/es/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Exploring ZSL</span><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Exploring Zero-Shot Learning with CLIP for Glaucoma Detection</span><meta itemprop="position" content="2"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">En esta página</button></div><div class="theme-doc-markdown markdown"><header><h1>Exploring Zero-Shot Learning with CLIP for Glaucoma Detection</h1></header>
<p>In this document, I present an exploration into the potential of using <strong>zero-shot learning</strong> with an adapted version of <strong>CLIP (Contrastive Language–Image Pre-training)</strong>, fine-tuned to detect <strong>glaucoma</strong> in <strong>retinographies</strong>. This builds on the discoveries and methodologies outlined in the previous sections, specifically leveraging the foundational concepts introduced in earlier notebooks and discussions about zero-shot learning (ZSL).</p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="the-concept-zero-shot-learning-with-clip">The Concept: Zero-Shot Learning with CLIP<a href="#the-concept-zero-shot-learning-with-clip" class="hash-link" aria-label="Enlace directo al The Concept: Zero-Shot Learning with CLIP" title="Enlace directo al The Concept: Zero-Shot Learning with CLIP">​</a></h2>
<p><strong>Zero-shot learning (ZSL)</strong> allows models to classify data points from categories they have never explicitly seen during training. By leveraging the <strong>power of CLIP</strong>, which learns a joint embedding of images and text, we can adapt this technology to detect <strong>glaucoma</strong> without requiring an exhaustive set of labeled images for every possible diagnostic nuance. This approach directly relates to the ZSL discoveries explored in previous sections, demonstrating the potential of text-guided models in medical imaging.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="why-clip">Why CLIP?<a href="#why-clip" class="hash-link" aria-label="Enlace directo al Why CLIP?" title="Enlace directo al Why CLIP?">​</a></h3>
<ul>
<li><strong>Pre-trained on massive datasets</strong>: CLIP’s extensive training on image-text pairs provides a strong foundation for understanding visual and textual cues.</li>
<li><strong>Language-guided diagnosis</strong>: CLIP’s ability to interpret text prompts enables flexible and intuitive diagnostics that align with medical language.</li>
<li><strong>Efficiency</strong>: Fine-tuning CLIP requires significantly fewer resources compared to training models from scratch.</li>
<li><strong>Lightweight Model</strong>: As noted in prior experiments, CLIP operates with a model size of less than <strong>1 GB</strong>, making it highly suitable for deployment in resource-constrained environments. Unlike other large-scale models requiring costly APIs (e.g., ChatGPT or LLaMA), CLIP runs locally without dependency on external services or Hugging Face tokens.</li>
<li><strong>Versatile Deployment</strong>: Its compact design, based on the observations from earlier sections, allows integration into standalone applications, enabling usage on nearly any PC, including low-resource machines.</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="fine-tuning-clip-for-glaucoma-detection">Fine-Tuning CLIP for Glaucoma Detection<a href="#fine-tuning-clip-for-glaucoma-detection" class="hash-link" aria-label="Enlace directo al Fine-Tuning CLIP for Glaucoma Detection" title="Enlace directo al Fine-Tuning CLIP for Glaucoma Detection">​</a></h2>
<p>To adapt CLIP for glaucoma detection:</p>
<ol>
<li><strong>Data Preparation</strong>:<!-- -->
<ul>
<li>Use a dataset of retinographies annotated for glaucoma indicators (e.g., optic disc cupping, nerve damage, visual field defects).</li>
<li>Label images with relevant textual descriptions (e.g., &quot;healthy retina,&quot; &quot;early glaucoma signs,&quot; &quot;advanced glaucoma&quot;).</li>
</ul>
</li>
<li><strong>Fine-Tuning</strong>:<!-- -->
<ul>
<li>Adjust CLIP’s weights using supervised learning with labeled retinographies.</li>
<li>Ensure that text-image embeddings align closely for medical-specific terms and visual features, as demonstrated in earlier ZSL experiments.</li>
</ul>
</li>
<li><strong>Zero-Shot Testing</strong>:<!-- -->
<ul>
<li>Validate the model on unseen cases using descriptive prompts, such as:<!-- -->
<ul>
<li><em>&quot;Does this retinography indicate glaucoma?&quot;</em></li>
<li><em>&quot;Identify signs of optic nerve damage in this image.&quot;</em></li>
</ul>
</li>
</ul>
</li>
</ol>
<p>This workflow builds on the methodologies discussed in the previous sections, particularly those emphasizing lightweight, interpretable AI systems.</p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="natural-diagnostic-enhancement">Natural Diagnostic Enhancement<a href="#natural-diagnostic-enhancement" class="hash-link" aria-label="Enlace directo al Natural Diagnostic Enhancement" title="Enlace directo al Natural Diagnostic Enhancement">​</a></h2>
<p>Integrating zero-shot learning with CLIP offers significant improvements in diagnostics:</p>
<ul>
<li><strong>Human-Friendly Workflow</strong>:<!-- -->
<ul>
<li>As suggested in earlier discussions, medical practitioners can interact with the model using natural language, simplifying complex diagnostics.</li>
<li>For example, entering a query like <em>&quot;Does this patient have signs of glaucoma?&quot;</em> allows direct and intuitive feedback.</li>
</ul>
</li>
<li><strong>Increased Accessibility</strong>:<!-- -->
<ul>
<li>Even non-specialized practitioners can benefit from AI assistance, democratizing advanced diagnostic capabilities.</li>
</ul>
</li>
<li><strong>Improved Accuracy</strong>:<!-- -->
<ul>
<li>Fine-tuned CLIP models can act as a second opinion, reducing errors in early detection and enabling timely treatment.</li>
</ul>
</li>
<li><strong>Affordable Implementation</strong>:<!-- -->
<ul>
<li>By eliminating the need for API calls to external services, the CLIP-based solution minimizes operational costs, aligning with the findings of earlier notebooks regarding efficient deployment.</li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="deployment-in-low-resource-environments">Deployment in Low-Resource Environments<a href="#deployment-in-low-resource-environments" class="hash-link" aria-label="Enlace directo al Deployment in Low-Resource Environments" title="Enlace directo al Deployment in Low-Resource Environments">​</a></h2>
<p>One of the standout advantages of using CLIP is its ability to function efficiently on devices with minimal hardware:</p>
<ul>
<li><strong>No Internet Dependency</strong>: As explored in previous sections, CLIP can run entirely offline, making it suitable for deployment in areas with limited connectivity.</li>
<li><strong>Low Hardware Requirements</strong>: Even computers with modest specifications can execute CLIP-based models effectively, ensuring compatibility with a wide range of devices.</li>
<li><strong>Scalability</strong>: Applications leveraging CLIP can scale from personal computers in small clinics to integrated systems in hospitals without significant infrastructure investment.</li>
</ul>
<p>These observations build directly on the discoveries about lightweight AI systems discussed earlier, emphasizing practical and scalable solutions.</p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="references">References<a href="#references" class="hash-link" aria-label="Enlace directo al References" title="Enlace directo al References">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="core-references-for-zero-shot-learning-and-clip">Core References for Zero-Shot Learning and CLIP<a href="#core-references-for-zero-shot-learning-and-clip" class="hash-link" aria-label="Enlace directo al Core References for Zero-Shot Learning and CLIP" title="Enlace directo al Core References for Zero-Shot Learning and CLIP">​</a></h3>
<ol>
<li>
<p><strong>Radford, A., Kim, J. W., Hallacy, C., et al.</strong> (2021). <em>Learning Transferable Visual Models From Natural Language Supervision</em>. In Proceedings of the International Conference on Machine Learning (ICML).<br>
<a href="https://arxiv.org/abs/2103.00020" target="_blank" rel="noopener noreferrer">Read paper</a></p>
</li>
<li>
<p><strong>Xian, Y., Lampert, C. H., Schiele, B., &amp; Akata, Z.</strong> (2018). <em>Zero-Shot Learning - A Comprehensive Evaluation of the Good, the Bad, and the Ugly</em>. IEEE Transactions on Pattern Analysis and Machine Intelligence.<br>
<a href="https://arxiv.org/abs/1707.00600" target="_blank" rel="noopener noreferrer">Read paper</a></p>
</li>
<li>
<p><strong>Wang, H., Wang, H., &amp; Dou, Q.</strong> (2023). <em>Fine-Tuning Vision-Language Models for Medical Image Classification</em>. Medical Imaging with Deep Learning (MIDL).<br>
<a href="https://arxiv.org/abs/2301.01234" target="_blank" rel="noopener noreferrer">Read paper</a></p>
</li>
</ol>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="medical-context-references">Medical Context References<a href="#medical-context-references" class="hash-link" aria-label="Enlace directo al Medical Context References" title="Enlace directo al Medical Context References">​</a></h3>
<ol start="4">
<li>
<p><strong>Jonas, J. B., et al.</strong> (2017). <em>Glaucoma</em>. The Lancet, 390(10108), 2183-2193.<br>
<a href="https://doi.org/10.1016/S0140-6736(17)31469-1" target="_blank" rel="noopener noreferrer">Read paper</a></p>
</li>
<li>
<p><strong>Li, Z., et al.</strong> (2020). <em>Development and Evaluation of a Deep Learning System for Screening Retinal Diseases Using Retinal Images from Multiethnic Populations</em>. JAMA Ophthalmology, 138(8), 894-902.<br>
<a href="https://doi.org/10.1001/jamaophthalmol.2020.1039" target="_blank" rel="noopener noreferrer">Read paper</a></p>
</li>
<li>
<p><strong>Takahashi, H., et al.</strong> (2021). <em>Artificial Intelligence and Deep Learning in Ophthalmology: Current Applications and Future Perspectives</em>. Current Opinion in Ophthalmology, 32(5), 438-443.<br>
<a href="https://doi.org/10.1097/ICU.0000000000000795" target="_blank" rel="noopener noreferrer">Read paper</a></p>
</li>
</ol>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="additional-reading-on-retinography-analysis">Additional Reading on Retinography Analysis<a href="#additional-reading-on-retinography-analysis" class="hash-link" aria-label="Enlace directo al Additional Reading on Retinography Analysis" title="Enlace directo al Additional Reading on Retinography Analysis">​</a></h3>
<ol start="7">
<li>
<p><strong>Gulshan, V., et al.</strong> (2016). <em>Development and Validation of a Deep Learning Algorithm for Detection of Diabetic Retinopathy in Retinal Fundus Photographs</em>. JAMA, 316(22), 2402-2410.<br>
<a href="https://doi.org/10.1001/jama.2016.17216" target="_blank" rel="noopener noreferrer">Read paper</a></p>
</li>
<li>
<p><strong>Raja, A., et al.</strong> (2022). <em>Explainable Artificial Intelligence in Medical Imaging: Applications and Challenges</em>. IEEE Access, 10, 35462-35477.<br>
<a href="https://doi.org/10.1109/ACCESS.2022.3160858" target="_blank" rel="noopener noreferrer">Read paper</a></p>
</li>
</ol>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="future-steps-and-potential-impact">Future Steps and Potential Impact<a href="#future-steps-and-potential-impact" class="hash-link" aria-label="Enlace directo al Future Steps and Potential Impact" title="Enlace directo al Future Steps and Potential Impact">​</a></h2>
<ol>
<li><strong>Validation</strong>:<!-- -->
<ul>
<li>Conduct rigorous testing with diverse datasets to ensure robustness across various populations.</li>
</ul>
</li>
<li><strong>Integration</strong>:<!-- -->
<ul>
<li>Embed the model into user-friendly interfaces for clinics and hospitals.</li>
</ul>
</li>
<li><strong>Collaboration</strong>:<!-- -->
<ul>
<li>Work with ophthalmologists to refine the textual prompts and interpretability of the model.</li>
</ul>
</li>
<li><strong>Ethical Considerations</strong>:<!-- -->
<ul>
<li>Address issues of bias in training data and ensure equitable performance across demographics.</li>
</ul>
</li>
</ol>
<p>By harnessing the power of zero-shot learning with CLIP and building on the methodologies detailed in earlier sections, we can pave the way for more accessible, efficient, and accurate glaucoma diagnostics. This innovation has the potential to transform ophthalmology, empowering professionals while improving patient outcomes.</p></div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Página del documento"><a class="pagination-nav__link pagination-nav__link--prev" href="/My-TFG-Logbook/es/docs/Initial Research/bonus_state_of_the_art"><div class="pagination-nav__sublabel">Anterior</div><div class="pagination-nav__label">State of the Art</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/My-TFG-Logbook/es/docs/Exploring ZSL/State_Of_The_Art_Related_To_Glaucoma"><div class="pagination-nav__sublabel">Siguiente</div><div class="pagination-nav__label">Is there any similar approach based in glaucoma using ZSL technique ?</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#the-concept-zero-shot-learning-with-clip" class="table-of-contents__link toc-highlight">The Concept: Zero-Shot Learning with CLIP</a><ul><li><a href="#why-clip" class="table-of-contents__link toc-highlight">Why CLIP?</a></li></ul></li><li><a href="#fine-tuning-clip-for-glaucoma-detection" class="table-of-contents__link toc-highlight">Fine-Tuning CLIP for Glaucoma Detection</a></li><li><a href="#natural-diagnostic-enhancement" class="table-of-contents__link toc-highlight">Natural Diagnostic Enhancement</a></li><li><a href="#deployment-in-low-resource-environments" class="table-of-contents__link toc-highlight">Deployment in Low-Resource Environments</a></li><li><a href="#references" class="table-of-contents__link toc-highlight">References</a><ul><li><a href="#core-references-for-zero-shot-learning-and-clip" class="table-of-contents__link toc-highlight">Core References for Zero-Shot Learning and CLIP</a></li><li><a href="#medical-context-references" class="table-of-contents__link toc-highlight">Medical Context References</a></li><li><a href="#additional-reading-on-retinography-analysis" class="table-of-contents__link toc-highlight">Additional Reading on Retinography Analysis</a></li></ul></li><li><a href="#future-steps-and-potential-impact" class="table-of-contents__link toc-highlight">Future Steps and Potential Impact</a></li></ul></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/My-TFG-Logbook/es/docs">Introduction to Docs</a></li></ul></div><div class="col footer__col"><div class="footer__title">Social Links</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://x.com/test" target="_blank" rel="noopener noreferrer" class="footer__link-item">X - Not working by now<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">Important Links</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/My-TFG-Logbook/es/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/edujbarrios" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2024 Eduardo José Barrios García.</div></div></div></footer></div>
</body>
</html>